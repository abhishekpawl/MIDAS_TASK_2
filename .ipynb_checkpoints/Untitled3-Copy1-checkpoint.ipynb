{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name:</b> Abhishek Paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\">Part 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>numpy</b> - to work with arrays.<br/>\n",
    "<b>cv2</b> - to work with images.<br/>\n",
    "<b>os</b> - to work with directories and for importing images.<br/>\n",
    "<b>random</b> - to randomly shuffle the sets.<br/>\n",
    "<b>pyplot</b> - library: <i>matplotlib</i> - to visualize the images.<br/>\n",
    "<b>tensorflow</b> - to make the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the path of the folder containing the training set images as a raw string in a variable 'dir'.<br/>We are storing it as a raw string because we <b>don't</b> want treat the <b>'\\\\'</b> characters as escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r'C:\\Users\\acer\\Desktop\\MIDAS_TASK_2\\train';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the names of the first ten folders containing - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "for i in range(0, 62):\n",
    "    class_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]\n"
     ]
    }
   ],
   "source": [
    "print(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the images and resizing the images to a shape of 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "\n",
    "for c in class_list:\n",
    "    folder = os.path.join(dir, str(c))\n",
    "    label = c\n",
    "    for img in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img)\n",
    "        img_arr = cv.imread(img_path, 0)\n",
    "        img_arr = cv.resize(img_arr, (64, 64))\n",
    "        data.append([img_arr, label]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently storing the image array and the categories(classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list()\n",
    "train_y = list()\n",
    "\n",
    "for img, label in data:\n",
    "    train_x.append(img)\n",
    "    train_y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the image list and the label list into numpy array for furthern operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding the classes as we will be using categorical cross-entropy as the loss function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tf.keras.utils.to_categorical(train_y, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2480, 64, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e8e78440c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the shape of the image array so as to fit in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.expand_dims(train_x, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the convolutional nueral network(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have built a fairly simple model because the dataset contains of numbers and alphabets instead of real world images and hence there will be less features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the convolutional layers to extract the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the first convolutional layer having 32 filters, a 3x3 kernel size and using the rectified linear unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the first convolutional layer having 64 filters, a 3x3 kernel size and using the rectified linear unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the first convolutional layer having 128 filters, a 3x3 kernel size and using the rectified linear unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the MaxPooling layer to downsample the convoluted arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a dropout layer to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a flattening layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the flattening layer to flatten the arrays before feeding it into the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the dense neural network having 128 neurons and using the Rectified Linear Unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the final output layer using the softmax function for categorical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(62, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing mini-batch gradient descent in batches of 64 images and 15 epochs, using 20% of the dataset as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1984 samples, validate on 496 samples\n",
      "Epoch 1/15\n",
      "1984/1984 [==============================] - 62s 31ms/sample - loss: 276.1119 - accuracy: 0.0207 - val_loss: 4.0956 - val_accuracy: 0.0323\n",
      "Epoch 2/15\n",
      "1984/1984 [==============================] - 64s 32ms/sample - loss: 3.7524 - accuracy: 0.1079 - val_loss: 3.5698 - val_accuracy: 0.1633\n",
      "Epoch 3/15\n",
      "1984/1984 [==============================] - 65s 33ms/sample - loss: 2.1561 - accuracy: 0.4556 - val_loss: 3.4317 - val_accuracy: 0.1935\n",
      "Epoch 4/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.8920 - accuracy: 0.7596 - val_loss: 4.4606 - val_accuracy: 0.2218\n",
      "Epoch 5/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.2892 - accuracy: 0.9249 - val_loss: 5.0418 - val_accuracy: 0.2319\n",
      "Epoch 6/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.1457 - accuracy: 0.9602 - val_loss: 5.8010 - val_accuracy: 0.2560\n",
      "Epoch 7/15\n",
      "1984/1984 [==============================] - 68s 34ms/sample - loss: 0.0801 - accuracy: 0.9829 - val_loss: 6.4012 - val_accuracy: 0.2440\n",
      "Epoch 8/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0662 - accuracy: 0.9803 - val_loss: 6.3246 - val_accuracy: 0.2460\n",
      "Epoch 9/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0620 - accuracy: 0.9844 - val_loss: 6.1430 - val_accuracy: 0.2359\n",
      "Epoch 10/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0824 - accuracy: 0.9839 - val_loss: 6.7034 - val_accuracy: 0.2278\n",
      "Epoch 11/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0587 - accuracy: 0.9854 - val_loss: 6.8495 - val_accuracy: 0.2581\n",
      "Epoch 12/15\n",
      "1984/1984 [==============================] - 68s 34ms/sample - loss: 0.0380 - accuracy: 0.9934 - val_loss: 6.8220 - val_accuracy: 0.2460\n",
      "Epoch 13/15\n",
      "1984/1984 [==============================] - 68s 34ms/sample - loss: 0.0209 - accuracy: 0.9955 - val_loss: 7.5759 - val_accuracy: 0.2621\n",
      "Epoch 14/15\n",
      "1984/1984 [==============================] - 68s 34ms/sample - loss: 0.0158 - accuracy: 0.9950 - val_loss: 7.5040 - val_accuracy: 0.2661\n",
      "Epoch 15/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0162 - accuracy: 0.9955 - val_loss: 7.3905 - val_accuracy: 0.2379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e88032a308>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_x, y=train_y, batch_size=64, validation_split=0.2, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\">Part 2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the image array and the label array for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the 'idx2numpy' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set = 'train-images.idx3-ubyte'\n",
    "trainarray = idx2numpy.convert_from_file(image_set)\n",
    "\n",
    "labels = 'train-labels.idx1-ubyte'\n",
    "labelarray = idx2numpy.convert_from_file(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the images into 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 64, 64), (60000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_images = list()\n",
    "\n",
    "for i in range(trainarray.shape[0]):\n",
    "    task_images.append(cv.resize(trainarray[i], (64, 64)))\n",
    "\n",
    "task_images = 255 - np.array(task_images)\n",
    "task_images.shape, labelarray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the shape of the image array so as to fit in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_images = np.expand_dims(task_images, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding the classes as we will be using categorical cross-entropy as the loss function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelarray = tf.keras.utils.to_categorical(labelarray, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the trained model on the standard mnist training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 1568s 33ms/sample - loss: 0.3715 - accuracy: 0.9050 - val_loss: 0.1098 - val_accuracy: 0.9683\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 1606s 33ms/sample - loss: 0.0745 - accuracy: 0.9778 - val_loss: 0.0991 - val_accuracy: 0.9713\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 1589s 33ms/sample - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0990 - val_accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 1585s 33ms/sample - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.1097 - val_accuracy: 0.9758\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 1578s 33ms/sample - loss: 0.0213 - accuracy: 0.9936 - val_loss: 0.1237 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e880063448>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=task_images, y=labelarray, batch_size=128, validation_split=0.2, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model was already trained, the model had some pre-initialized weights, we can say, it had the idea and so the convergence time was pretty quick.<br/>\n",
    "In <i style=\"color:green\">Part 1:</i> validation accuracy was low.<br/>\n",
    "In <i style=\"color:green\">Part 2:</i> validation accuracy was much higher.<br/>\n",
    "### The model has not much <b style=\"color:green\">overfitted</b> on the training data as the training acccuracy and validation accuracy are close as well as the training set loss and the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 't10k-images.idx3-ubyte'\n",
    "test_x = idx2numpy.convert_from_file(test)\n",
    "\n",
    "testlabels = 't10k-labels.idx1-ubyte'\n",
    "test_y = idx2numpy.convert_from_file(testlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the images into 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 64, 64), (10000,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = list()\n",
    "\n",
    "for i in range(test_x.shape[0]):\n",
    "    test_images.append(cv.resize(test_x[i], (64, 64)))\n",
    "\n",
    "test_images = 255 - np.array(test_images)\n",
    "test_images.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the shape of the image array so as to fit in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding the classes as we will be using categorical cross-entropy as the loss function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = tf.keras.utils.to_categorical(test_y, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 59s 6ms/sample - loss: 0.1128 - accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "results = cnn.evaluate(test_images, test_y, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved an accuracy of 97.42% on the standard MNIST test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\">Part 3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided link: https://www.dropbox.com/s/otc12z2w7f7xm8z/mnistTask3.zip<br/>\n",
    "I handpicked hundreds of each of the 10 digits and moved them to individual folders to organize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the images for Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r'C:\\Users\\acer\\Desktop\\MIDAS_TASK_2\\mnist_task';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3 = list()\n",
    "\n",
    "for c in class_list[:10]:\n",
    "    folder = os.path.join(dir, str(c))\n",
    "    label = c\n",
    "    for img in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img)\n",
    "        img_arr = cv.imread(img_path, 0)\n",
    "        img_arr = cv.resize(img_arr, (64, 64))\n",
    "        part3.append([img_arr, label]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(part3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = list()\n",
    "y_3 = list()\n",
    "\n",
    "for img, label in data:\n",
    "    x_3.append(img)\n",
    "    y_3.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = np.array(x_3)\n",
    "y_3 = np.array(y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = tf.keras.utils.to_categorical(y_3, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = np.expand_dims(x_3, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a similar CNN from scratch with random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2 = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the second convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the third convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the MaxPooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the flattening layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the layer of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Dense(62, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the new sample of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1984 samples, validate on 496 samples\n",
      "Epoch 1/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 375.8255 - accuracy: 0.0207 - val_loss: 4.1185 - val_accuracy: 0.0383\n",
      "Epoch 2/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 3.7227 - accuracy: 0.1074 - val_loss: 3.6698 - val_accuracy: 0.1431\n",
      "Epoch 3/15\n",
      "1984/1984 [==============================] - 69s 35ms/sample - loss: 2.3284 - accuracy: 0.4128 - val_loss: 3.4715 - val_accuracy: 0.2319\n",
      "Epoch 4/15\n",
      "1984/1984 [==============================] - 68s 34ms/sample - loss: 1.1180 - accuracy: 0.7031 - val_loss: 3.7546 - val_accuracy: 0.2681\n",
      "Epoch 5/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.4515 - accuracy: 0.8831 - val_loss: 4.6283 - val_accuracy: 0.2641\n",
      "Epoch 6/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.1939 - accuracy: 0.9536 - val_loss: 5.0475 - val_accuracy: 0.2480\n",
      "Epoch 7/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.1073 - accuracy: 0.9743 - val_loss: 5.3915 - val_accuracy: 0.2399\n",
      "Epoch 8/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0728 - accuracy: 0.9849 - val_loss: 5.7984 - val_accuracy: 0.2762\n",
      "Epoch 9/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0501 - accuracy: 0.9884 - val_loss: 6.3642 - val_accuracy: 0.2762\n",
      "Epoch 10/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.0313 - accuracy: 0.9919 - val_loss: 6.7659 - val_accuracy: 0.2681\n",
      "Epoch 11/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.0332 - accuracy: 0.9919 - val_loss: 6.5861 - val_accuracy: 0.2702\n",
      "Epoch 12/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.0280 - accuracy: 0.9934 - val_loss: 6.8028 - val_accuracy: 0.2480\n",
      "Epoch 13/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.0257 - accuracy: 0.9934 - val_loss: 6.6683 - val_accuracy: 0.2702\n",
      "Epoch 14/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 0.0250 - accuracy: 0.9950 - val_loss: 6.5391 - val_accuracy: 0.2742\n",
      "Epoch 15/15\n",
      "1984/1984 [==============================] - 65s 33ms/sample - loss: 0.0096 - accuracy: 0.9985 - val_loss: 7.5634 - val_accuracy: 0.2681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e883f9a588>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_2.fit(x=x_3, y=y_3, batch_size=100, validation_split=0.2, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the standard MNIST test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 57s 6ms/sample - loss: 12.6321 - accuracy: 0.0278\n"
     ]
    }
   ],
   "source": [
    "results_2 = cnn_2.evaluate(test_images, test_y, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0278\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", results_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second CNN, that was trained using scratch random initialization on the small dataset performed poorly and had a very low accuracy. The network overfitted on the training set.\n",
    "- The network was very small.\n",
    "- The network's training process was short.  \n",
    "\n",
    "The accuracy of the second CNN was very low compared to the CNN used in <b style=\"color:green\">Part 2</b> as that was trained on a relatively larger dataset and for a much longer time.\n",
    "\n",
    "There were shortcomings also due to the computational power available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
