{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name:</b> Abhishek Paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\">Part 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>numpy</b> - to work with arrays.<br/>\n",
    "<b>cv2</b> - to work with images.<br/>\n",
    "<b>os</b> - to work with directories and for importing images.<br/>\n",
    "<b>random</b> - to randomly shuffle the sets.<br/>\n",
    "<b>pyplot</b> - library: <i>matplotlib</i> - to visualize the images.<br/>\n",
    "<b>tensorflow</b> - to make the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the path of the folder containing the training set images as a raw string in a variable 'dir'.<br/>We are storing it as a raw string because we <b>don't</b> want treat the <b>'\\\\'</b> characters as escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r'C:\\Users\\acer\\Desktop\\MIDAS_TASK_2\\train';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the names of the first ten folders containing - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "for i in range(0, 62):\n",
    "    class_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]\n"
     ]
    }
   ],
   "source": [
    "print(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the images and resizing the images to a shape of 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "\n",
    "for c in class_list:\n",
    "    folder = os.path.join(dir, str(c))\n",
    "    label = c\n",
    "    for img in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img)\n",
    "        img_arr = cv.imread(img_path, 0)\n",
    "        img_arr = cv.resize(img_arr, (64, 64))\n",
    "        data.append([img_arr, label]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently storing the image array and the categories(classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list()\n",
    "train_y = list()\n",
    "\n",
    "for img, label in data:\n",
    "    train_x.append(img)\n",
    "    train_y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the image list and the label list into numpy array for furthern operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding the classes as we will be using categorical cross-entropy as the loss function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tf.keras.utils.to_categorical(train_y, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2480, 64, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a70b212248>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the shape of the image array so as to fit in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.expand_dims(train_x, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the convolutional nueral network(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have built a fairly simple model because the dataset contains of numbers and alphabets instead of real world images and hence there will be less features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the convolutional layers to extract the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the first convolutional layer having 32 filters, a 3x3 kernel size and using the rectified linear unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the first convolutional layer having 64 filters, a 3x3 kernel size and using the rectified linear unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the first convolutional layer having 128 filters, a 3x3 kernel size and using the rectified linear unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the MaxPooling layer to downsample the convoluted arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a dropout layer to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a flattening layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the flattening layer to flatten the arrays before feeding it into the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the dense neural network having 128 neurons and using the Rectified Linear Unit(ReLU) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the final output layer using the softmax function for categorical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(62, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing mini-batch gradient descent in batches of 64 images and 15 epochs, using 20% of the dataset as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1984 samples, validate on 496 samples\n",
      "Epoch 1/15\n",
      "1984/1984 [==============================] - 68s 34ms/sample - loss: 262.4840 - accuracy: 0.0131 - val_loss: 4.0983 - val_accuracy: 0.0363\n",
      "Epoch 2/15\n",
      "1984/1984 [==============================] - 69s 35ms/sample - loss: 3.8455 - accuracy: 0.0958 - val_loss: 3.6222 - val_accuracy: 0.1492\n",
      "Epoch 3/15\n",
      "1984/1984 [==============================] - 66s 33ms/sample - loss: 2.3058 - accuracy: 0.4229 - val_loss: 3.8976 - val_accuracy: 0.2520\n",
      "Epoch 4/15\n",
      "1984/1984 [==============================] - 67s 34ms/sample - loss: 0.9787 - accuracy: 0.7303 - val_loss: 4.4876 - val_accuracy: 0.2540\n",
      "Epoch 5/15\n",
      "1984/1984 [==============================] - 71s 36ms/sample - loss: 0.4058 - accuracy: 0.8982 - val_loss: 5.3716 - val_accuracy: 0.2278\n",
      "Epoch 6/15\n",
      "1984/1984 [==============================] - 71s 36ms/sample - loss: 0.2301 - accuracy: 0.9410 - val_loss: 5.7716 - val_accuracy: 0.2581\n",
      "Epoch 7/15\n",
      "1984/1984 [==============================] - 71s 36ms/sample - loss: 0.1398 - accuracy: 0.9682 - val_loss: 6.3292 - val_accuracy: 0.2581\n",
      "Epoch 8/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.1166 - accuracy: 0.9703 - val_loss: 6.3185 - val_accuracy: 0.2359\n",
      "Epoch 9/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0976 - accuracy: 0.9793 - val_loss: 6.9859 - val_accuracy: 0.2520\n",
      "Epoch 10/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0845 - accuracy: 0.9819 - val_loss: 7.1937 - val_accuracy: 0.2802\n",
      "Epoch 11/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0570 - accuracy: 0.9904 - val_loss: 7.9427 - val_accuracy: 0.2681\n",
      "Epoch 12/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0638 - accuracy: 0.9854 - val_loss: 7.7863 - val_accuracy: 0.2581\n",
      "Epoch 13/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0707 - accuracy: 0.9824 - val_loss: 7.0096 - val_accuracy: 0.2581\n",
      "Epoch 14/15\n",
      "1984/1984 [==============================] - 71s 36ms/sample - loss: 0.0747 - accuracy: 0.9798 - val_loss: 7.5201 - val_accuracy: 0.2702\n",
      "Epoch 15/15\n",
      "1984/1984 [==============================] - 70s 35ms/sample - loss: 0.0641 - accuracy: 0.9859 - val_loss: 7.5211 - val_accuracy: 0.2601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a70914f8c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_x, y=train_y, batch_size=64, validation_split=0.2, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\">Part 2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the image array and the label array for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the 'idx2numpy' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set = 'train-images.idx3-ubyte'\n",
    "trainarray = idx2numpy.convert_from_file(image_set)\n",
    "\n",
    "labels = 'train-labels.idx1-ubyte'\n",
    "labelarray = idx2numpy.convert_from_file(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the images into 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 64, 64), (60000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_images = list()\n",
    "\n",
    "for i in range(trainarray.shape[0]):\n",
    "    task_images.append(cv.resize(trainarray[i], (64, 64)))\n",
    "\n",
    "task_images = 255 - np.array(task_images)\n",
    "task_images.shape, labelarray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the shape of the image array so as to fit in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_images = np.expand_dims(task_images, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding the classes as we will be using categorical cross-entropy as the loss function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelarray = tf.keras.utils.to_categorical(labelarray, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the trained model on the standard mnist training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 1643s 34ms/sample - loss: 1.1114 - accuracy: 0.7705 - val_loss: 0.2996 - val_accuracy: 0.9277\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 1630s 34ms/sample - loss: 0.2063 - accuracy: 0.9384 - val_loss: 0.1865 - val_accuracy: 0.9488\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 1617s 34ms/sample - loss: 0.1170 - accuracy: 0.9639 - val_loss: 0.1543 - val_accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 1631s 34ms/sample - loss: 0.0936 - accuracy: 0.9705 - val_loss: 0.1616 - val_accuracy: 0.9547\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 1634s 34ms/sample - loss: 0.0633 - accuracy: 0.9797 - val_loss: 0.2149 - val_accuracy: 0.9597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a71997b948>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=task_images, y=labelarray, batch_size=128, validation_split=0.2, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model was already trained, the model had some pre-initialized weights, we can say, it had the idea and so the convergence time was pretty quick.<br/>\n",
    "In <i style=\"color:green\">Part 1:</i> validation accuracy was low.<br/>\n",
    "In <i style=\"color:green\">Part 2:</i> validation accuracy was much higher.<br/>\n",
    "### The model has not much <b style=\"color:green\">overfitted</b> on the training data as the training acccuracy and validation accuracy are close as well as the training set loss and the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 't10k-images.idx3-ubyte'\n",
    "test_x = idx2numpy.convert_from_file(test)\n",
    "\n",
    "testlabels = 't10k-labels.idx1-ubyte'\n",
    "test_y = idx2numpy.convert_from_file(testlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the images into 64x64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 64, 64), (10000,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = list()\n",
    "\n",
    "for i in range(test_x.shape[0]):\n",
    "    test_images.append(cv.resize(test_x[i], (64, 64)))\n",
    "\n",
    "test_images = 255 - np.array(test_images)\n",
    "test_images.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the shape of the image array so as to fit in the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding the classes as we will be using categorical cross-entropy as the loss function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = tf.keras.utils.to_categorical(test_y, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 58s 6ms/sample - loss: 0.2225 - accuracy: 0.9587\n"
     ]
    }
   ],
   "source": [
    "results = cnn.evaluate(test_images, test_y, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9587\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved an accuracy of 95.87% on the standard MNIST test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\">Part 3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided link: https://www.dropbox.com/s/otc12z2w7f7xm8z/mnistTask3.zip<br/>\n",
    "I handpicked hundreds of each of the 10 digits and moved them to individual folders to organize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the images for Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r'C:\\Users\\acer\\Desktop\\MIDAS_TASK_2\\mnist_task';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3 = list()\n",
    "\n",
    "for c in class_list[:10]:\n",
    "    folder = os.path.join(dir, str(c))\n",
    "    label = c\n",
    "    for img in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img)\n",
    "        img_arr = cv.imread(img_path, 0)\n",
    "        img_arr = cv.resize(img_arr, (64, 64))\n",
    "        part3.append([img_arr, label]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(part3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = list()\n",
    "y_3 = list()\n",
    "\n",
    "for img, label in part3:\n",
    "    x_3.append(img)\n",
    "    y_3.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = np.array(x_3)\n",
    "y_3 = np.array(y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = tf.keras.utils.to_categorical(y_3, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = np.expand_dims(x_3, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a similar CNN from scratch with random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2 = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the second convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the third convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the MaxPooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the flattening layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the layer of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.add(tf.keras.layers.Dense(62, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the new sample of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 778 samples, validate on 195 samples\n",
      "Epoch 1/15\n",
      "778/778 [==============================] - 26s 34ms/sample - loss: 122.9115 - accuracy: 0.2648 - val_loss: 2.1949 - val_accuracy: 0.5692\n",
      "Epoch 2/15\n",
      "778/778 [==============================] - 27s 35ms/sample - loss: 1.2815 - accuracy: 0.6928 - val_loss: 0.7230 - val_accuracy: 0.8256\n",
      "Epoch 3/15\n",
      "778/778 [==============================] - 30s 39ms/sample - loss: 0.3666 - accuracy: 0.9010 - val_loss: 0.5307 - val_accuracy: 0.8872\n",
      "Epoch 4/15\n",
      "778/778 [==============================] - 28s 36ms/sample - loss: 0.0841 - accuracy: 0.9730 - val_loss: 0.4967 - val_accuracy: 0.8974\n",
      "Epoch 5/15\n",
      "778/778 [==============================] - 29s 37ms/sample - loss: 0.0280 - accuracy: 0.9936 - val_loss: 0.5582 - val_accuracy: 0.9128\n",
      "Epoch 6/15\n",
      "778/778 [==============================] - 28s 36ms/sample - loss: 0.0047 - accuracy: 0.9974 - val_loss: 0.6722 - val_accuracy: 0.9231\n",
      "Epoch 7/15\n",
      "778/778 [==============================] - 26s 34ms/sample - loss: 0.0029 - accuracy: 0.9987 - val_loss: 0.7045 - val_accuracy: 0.9179\n",
      "Epoch 8/15\n",
      "778/778 [==============================] - 26s 34ms/sample - loss: 5.6433e-04 - accuracy: 1.0000 - val_loss: 0.7850 - val_accuracy: 0.9179\n",
      "Epoch 9/15\n",
      "778/778 [==============================] - 27s 34ms/sample - loss: 3.2396e-04 - accuracy: 1.0000 - val_loss: 0.8255 - val_accuracy: 0.9231\n",
      "Epoch 10/15\n",
      "778/778 [==============================] - 26s 34ms/sample - loss: 1.7651e-04 - accuracy: 1.0000 - val_loss: 0.8358 - val_accuracy: 0.9231\n",
      "Epoch 11/15\n",
      "778/778 [==============================] - 28s 36ms/sample - loss: 1.0231e-04 - accuracy: 1.0000 - val_loss: 0.8402 - val_accuracy: 0.9231\n",
      "Epoch 12/15\n",
      "778/778 [==============================] - 28s 36ms/sample - loss: 8.0279e-05 - accuracy: 1.0000 - val_loss: 0.8458 - val_accuracy: 0.9282\n",
      "Epoch 13/15\n",
      "778/778 [==============================] - 28s 35ms/sample - loss: 5.5196e-05 - accuracy: 1.0000 - val_loss: 0.8509 - val_accuracy: 0.9282\n",
      "Epoch 14/15\n",
      "778/778 [==============================] - 27s 35ms/sample - loss: 6.7231e-05 - accuracy: 1.0000 - val_loss: 0.8559 - val_accuracy: 0.9231\n",
      "Epoch 15/15\n",
      "778/778 [==============================] - 27s 34ms/sample - loss: 4.4462e-05 - accuracy: 1.0000 - val_loss: 0.8644 - val_accuracy: 0.9231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a71bfb2a08>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_2.fit(x=x_3, y=y_3, batch_size=100, validation_split=0.2, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the standard MNIST test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 55s 5ms/sample - loss: 0.6853 - accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "results_2 = cnn_2.evaluate(255 - test_images, test_y, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", results_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second CNN had an accuracy of 90.48% on the standard MNIST test split.\n",
    "\n",
    "For the second CNN, that was trained using scratch random initialization on the small dataset had a lower accuracy.\n",
    "This CNN was trained on a fairly smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function graph: For the first model trained on the training dataset provided in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "ys1 = np.array([262.4840, 3.8455, 2.3058, 0.9787, 0.4058, 0.2301, 0.1398, 0.1166, 0.0976, 0.0845, 0.0570, 0.0638, 0.0707, 0.0747, 0.0641])\n",
    "\n",
    "plt.plot(xs1, ys1)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function graph: For the second model trained on the training dataset provided in part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs2 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "ys2 = np.array([122.9115, 1.2815, 0.3666, 0.0841, 0.0280, 0.0047, 0.0029, 0.00056, 0.00032, 0.00017, 0.00010, 0.00008, 0.000055, 0.000067, 0.000047])\n",
    "\n",
    "plt.plot(xs2, ys2)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset provided in part 1, there were 62 types of characters including alphabets also, along with the digits and so the validation accuracy was very low because there must have been many cases where 'i' must have been considered '1' or '0'(zero) must have been considered 'o' and many such cases.\n",
    "\n",
    "But, for the dataset provided in part 2, there were only digits and so the validation accuracy was much higher and also the convergence time was much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "For commands, <b style=\"color:green\">tensorflow documentation</b> and <b style=\"color:green\">stackoverflow</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
